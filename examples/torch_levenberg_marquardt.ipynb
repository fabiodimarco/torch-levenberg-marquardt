{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "1FAjo3O1Q8W4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx5fBx95QDky"
      },
      "outputs": [],
      "source": [
        "!pip install torch-levenberg-marquardt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curve fitting example\n",
        "\n",
        "A simple curve-fitting example is implemented in `examples/sinc_curve_fitting.py` and `examples/sinc_curve_fitting_lightning.py`. The function `y = sinc(10 * x)` is fitted using a shallow neural network with 61 parameters.\n",
        "Despite the simplicity of the problem, first-order methods such as Adam fail to converge, whereas Levenberg-Marquardt converges rapidly with very low loss values. The learning rate values were chosen experimentally based on the results obtained by each algorithm."
      ],
      "metadata": {
        "id": "Qk96Tf4RQ9K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch_levenberg_marquardt as tlm\n",
        "from bokeh.plotting import figure, output_notebook, show\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Set PyTorch to use high precision for matrix multiplication\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Detect CUDA device for acceleration\n",
        "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(accelerator)\n",
        "devices = 1\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# %%\n",
        "# Generate synthetic dataset for training\n",
        "input_size = 20000  # Total number of data points\n",
        "batch_size = 1000  # Number of samples per batch\n",
        "\n",
        "# Generate training inputs and outputs (y = sinc(10 * x))\n",
        "x_train = torch.linspace(-1, 1, input_size, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "y_train = torch.sinc(10 * x_train).to(device)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# %%\n",
        "# Define a function to create the neural network model\n",
        "def create_model() -> torch.nn.Module:\n",
        "    \"\"\"Creates a simple feedforward neural network.\"\"\"\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Linear(1, 20),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(20, 1),\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# Initialize models for Adam and Levenberg-Marquardt optimization\n",
        "model = create_model()\n",
        "model_lm = create_model()\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "num_parameters = sum(p.numel() for p in model_lm.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {num_parameters}')\n",
        "\n",
        "# %%\n",
        "# Wrap models in CustomLightningModules with their respective optimization strategies\n",
        "module = tlm.utils.CustomLightningModule(\n",
        "    tlm.training.OptimizerModule(\n",
        "        model=model,\n",
        "        optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
        "        loss_fn=torch.nn.MSELoss(),\n",
        "    ),\n",
        ").to(device)\n",
        "\n",
        "module_lm = tlm.utils.CustomLightningModule(\n",
        "    tlm.training.LevenbergMarquardtModule(\n",
        "        model=model_lm,\n",
        "        loss_fn=tlm.loss.MSELoss(),\n",
        "        learning_rate=1.0,\n",
        "        attempts_per_step=10,\n",
        "        solve_method='solve',\n",
        "        # jacobian_max_num_rows=100,  # Uncomment if memory optimization is needed\n",
        "    )\n",
        ").to(device)\n",
        "\n",
        "# %%\n",
        "# Create PyTorch Lightning Trainers\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=False,\n",
        "    enable_checkpointing=False,\n",
        "    enable_model_summary=False,\n",
        ")\n",
        "\n",
        "trainer_lm = pl.Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=False,\n",
        "    enable_checkpointing=False,\n",
        "    enable_model_summary=False,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Train the model using the Adam optimizer\n",
        "print('Training with Adam optimizer...')\n",
        "t1_start = time.perf_counter()\n",
        "trainer.fit(module, train_loader)\n",
        "t1_stop = time.perf_counter()\n",
        "print(f'Training completed. Elapsed time: {t1_stop - t1_start:.2f} seconds')\n",
        "\n",
        "# %%\n",
        "# Train the model using the Levenberg-Marquardt algorithm\n",
        "print('Training with Levenberg-Marquardt...')\n",
        "t2_start = time.perf_counter()\n",
        "trainer_lm.fit(module_lm, train_loader)\n",
        "t2_stop = time.perf_counter()\n",
        "print(f'Training completed. Elapsed time: {t2_stop - t2_start:.2f} seconds')\n",
        "\n",
        "# %%\n",
        "# Evaluate both models on the training set and plot the results\n",
        "print('Generating predictions and plotting results...')\n",
        "\n",
        "# Generate predictions for the entire training dataset\n",
        "with torch.no_grad():\n",
        "    y_pred_adam = model(x_train)\n",
        "    y_pred_lm = model_lm(x_train)\n",
        "\n",
        "# Activate notebook output for Bokeh plots\n",
        "output_notebook()\n",
        "\n",
        "# Flatten tensors for plotting\n",
        "x_train_np = x_train.cpu().numpy().flatten()\n",
        "y_train_np = y_train.cpu().numpy().flatten()\n",
        "y_pred_adam_np = y_pred_adam.cpu().numpy().flatten()\n",
        "y_pred_lm_np = y_pred_lm.cpu().numpy().flatten()\n",
        "\n",
        "# Create a Bokeh figure\n",
        "p = figure(\n",
        "    title='Comparison of Optimization Methods',\n",
        "    x_axis_label='x_train',\n",
        "    y_axis_label='y_values',\n",
        "    width=800,\n",
        "    height=400,\n",
        ")\n",
        "\n",
        "# Add reference and predictions to the plot\n",
        "p.line(x_train_np, y_train_np, line_width=2, color='blue', legend_label='Reference')\n",
        "p.line(\n",
        "    x_train_np,\n",
        "    y_pred_adam_np,\n",
        "    line_width=2,\n",
        "    line_dash='dashed',\n",
        "    color='green',\n",
        "    legend_label='Adam',\n",
        ")\n",
        "p.line(\n",
        "    x_train_np,\n",
        "    y_pred_lm_np,\n",
        "    line_width=2,\n",
        "    line_dash='dashed',\n",
        "    color='red',\n",
        "    legend_label='Levenberg-Marquardt',\n",
        ")\n",
        "\n",
        "# Customize legend\n",
        "p.legend.title = 'Methods'\n",
        "p.legend.label_text_font_size = '10pt'\n",
        "p.legend.location = 'top_left'\n",
        "\n",
        "# Display the plot in the notebook\n",
        "show(p)"
      ],
      "metadata": {
        "id": "vyv-VrLSQ-KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST dataset clasification example\n",
        "A common MNIST classification example is implemented in `examples/mnist_classification.py.py` and `examples/mnist_classification_lightning.py.py`. The classification is performed using a convolutional neural network with 1026 parameters.\n",
        "Both optimization methods achieve roughly the same accuracy on the training and test sets; however, Levenberg-Marquardt requires significantly fewer epochs, automatically stopping the training at epoch 8."
      ],
      "metadata": {
        "id": "QWeMub7fQ-vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch_levenberg_marquardt as tlm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Set PyTorch to use high precision for matrix multiplication\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# Detect CUDA device for acceleration\n",
        "accelerator = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(accelerator)\n",
        "devices = 1\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# %%\n",
        "# Load MNIST dataset with transformations\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "batch_size = 5000  # Number of samples per batch\n",
        "\n",
        "# Load training and test datasets\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Create DataLoaders with updated num_workers for parallelism\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "\n",
        "# %%\n",
        "# Define a function to create the convolutional neural network model\n",
        "def create_conv_model() -> torch.nn.Module:\n",
        "    \"\"\"Creates a convolutional neural network for MNIST classification.\"\"\"\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(1, 8, kernel_size=4, stride=2, padding=0),  # (8, 13, 13)\n",
        "        torch.nn.ELU(),\n",
        "        torch.nn.Conv2d(8, 4, kernel_size=4, stride=2, padding=0),  # (4, 5, 5)\n",
        "        torch.nn.ELU(),\n",
        "        torch.nn.Conv2d(4, 4, kernel_size=2, stride=1, padding=0),  # (4, 4, 4)\n",
        "        torch.nn.ELU(),\n",
        "        torch.nn.Conv2d(4, 4, kernel_size=2, stride=1, padding=0),  # (4, 3, 3)\n",
        "        torch.nn.ELU(),\n",
        "        torch.nn.Conv2d(4, 4, kernel_size=2, stride=1, padding=0),  # (4, 2, 2)\n",
        "        torch.nn.ELU(),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Linear(4 * 2 * 2, 10),  # Fully connected layer for 10 classes\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "# Initialize models for Adam and Levenberg-Marquardt optimization\n",
        "model = create_conv_model()\n",
        "model_lm = create_conv_model()\n",
        "\n",
        "# Print the number of trainable parameters\n",
        "num_parameters = sum(p.numel() for p in model_lm.parameters() if p.requires_grad)\n",
        "print(f'Number of trainable parameters: {num_parameters}')\n",
        "\n",
        "# %%\n",
        "# Wrap models in CustomLightningModules with their respective optimization strategies\n",
        "module = tlm.utils.CustomLightningModule(\n",
        "    tlm.training.OptimizerModule(\n",
        "        model=model,\n",
        "        optimizer=torch.optim.Adam(model.parameters(), lr=0.01),\n",
        "        loss_fn=torch.nn.CrossEntropyLoss(),\n",
        "    ),\n",
        "    metrics={'accuracy': Accuracy(task='multiclass', num_classes=10)},\n",
        ").to(device)\n",
        "\n",
        "module_lm = tlm.utils.CustomLightningModule(\n",
        "    tlm.training.LevenbergMarquardtModule(\n",
        "        model=model_lm,\n",
        "        loss_fn=tlm.loss.CrossEntropyLoss(),\n",
        "        learning_rate=0.05,\n",
        "        attempts_per_step=10,\n",
        "        solve_method='qr',\n",
        "        jacobian_max_num_rows=200,  # Optimize memory usage for large datasets\n",
        "    ),\n",
        "    metrics={'accuracy': Accuracy(task='multiclass', num_classes=10)},\n",
        ").to(device)\n",
        "\n",
        "# Create PyTorch Lightning Trainers\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=False,\n",
        "    enable_checkpointing=False,\n",
        "    enable_model_summary=False,\n",
        ")\n",
        "\n",
        "trainer_lm = pl.Trainer(\n",
        "    max_epochs=10,\n",
        "    accelerator=accelerator,\n",
        "    devices=devices,\n",
        "    logger=False,\n",
        "    enable_checkpointing=False,\n",
        "    enable_model_summary=False,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Train the model using the Adam optimizer\n",
        "print('Training with Adam optimizer...')\n",
        "t1_start = time.perf_counter()\n",
        "trainer.fit(module, train_loader)\n",
        "t1_stop = time.perf_counter()\n",
        "print(f'Training completed. Elapsed time: {t1_stop - t1_start:.2f} seconds')\n",
        "\n",
        "# %%\n",
        "# Train the model using the Levenberg-Marquardt algorithm\n",
        "print('Training with Levenberg-Marquardt...')\n",
        "t2_start = time.perf_counter()\n",
        "trainer_lm.fit(module_lm, train_loader)\n",
        "t2_stop = time.perf_counter()\n",
        "print(f'Training completed. Elapsed time: {t2_stop - t2_start:.2f} seconds')\n",
        "\n",
        "\n",
        "# %%\n",
        "# Define evaluation function for the model\n",
        "def evaluate_model(model, data_loader):\n",
        "    \"\"\"Evaluates the model on the provided dataset.\n",
        "\n",
        "    Args:\n",
        "        model: The trained model to evaluate.\n",
        "        data_loader: DataLoader providing the dataset for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Average loss and accuracy on the dataset.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    accuracy_metric = Accuracy(task='multiclass', num_classes=10).to(device)\n",
        "    total_loss = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            # Move data to the appropriate device\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            # Compute loss for the batch\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                y_pred, y_batch, reduction='sum'\n",
        "            )  # Sum loss over the batch\n",
        "            total_loss += loss.item()\n",
        "            total_samples += y_batch.size(0)\n",
        "\n",
        "            # Update accuracy metric\n",
        "            accuracy_metric.update(y_pred, y_batch)\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = accuracy_metric.compute().item()\n",
        "\n",
        "    # Reset metric for future use\n",
        "    accuracy_metric.reset()\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "# %%\n",
        "# Evaluate both models and print results\n",
        "adam_loss, adam_acc = evaluate_model(module, test_loader)\n",
        "lm_loss, lm_acc = evaluate_model(module_lm, test_loader)\n",
        "\n",
        "print(f'Adam - Test Loss: {adam_loss:.6f}, Test Accuracy: {adam_acc:.2%}')\n",
        "print(f'Levenberg-Marquardt - Test Loss: {lm_loss:.6f}, Test Accuracy: {lm_acc:.2%}')"
      ],
      "metadata": {
        "id": "BGM-bsg4Q_Jq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}